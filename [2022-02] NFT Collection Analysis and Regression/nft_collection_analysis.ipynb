{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EDA, Feature Engineering, Hypothesis Testing, and Classification on NFT Collection Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "\n",
    "##### What is Niftyprices?\n",
    "Niftyprice is a new and small team focused on providing the most up to date and comprehensive NFT data in the market today. As you know, this is a fast moving space, so we are working hard to continually push updates, increase our coverage, and enhance our data engine to provide you the best product possible while staying up to speed on all the rapid changes taking place in the NFT world. At the end of the day, we're here for you, the NFT investors and patrons, so please let us know any feedback you have or how we can help solve your burning NFT issues. -NP [(source)](https://www.niftyprice.io/about)\n",
    "\n",
    "##### What are NFT's?\n",
    "A non-fungible token is a unit of data stored on a digital ledger, called a blockchain, that certifies a digital asset to be unique and therefore not interchangeable. NFTs can be used to represent items such as photos, videos, audio, and other types of digital files. [(source)](https://www.niftyprice.io/about)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Overview**\n",
    "\n",
    "##### Feature Information\n",
    "\n",
    "* Collection Name: The name of the NFT collection.\n",
    "* Floor Purchase Price: The lowest price of any NFT in the collection in Ethereum (ETH).\n",
    "* 24%: The percentage of floor price's moving values per 24 hours.\n",
    "* 7d%: The percentage of floor price's moving values per 7 days.\n",
    "* Total Float: The toral amount of minted NFTs.\n",
    "* Floor Cap: The lowest market capitalization—total value of the collection's items in circulation—in in Ethereum (ETH).\n",
    "* Volume: The volume of sales from the NFT collection in Ethereum (ETH) per 24 hours.\n",
    "* 24h Volume%: The percentage of volume's moving values per 24 hours.\n",
    "* Sales: The number of sales from the NFT collection.\n",
    "* 24h Owners%: The ownership percentage of all items in the collection per 24 hours.\n",
    "* %Float: The percentage of listed NFTs.\n",
    "* 24h supply%: The percentage of supply's moving values per 24 hours.\n",
    "* image_url: The associated image of the NFT collection.\n",
    "\n",
    "##### Source\n",
    "\n",
    "Dataset was scraped from [niftyprices](https://www.kaggle.com/fedesoriano/heart-failure-prediction) on February 20, 2022.\n",
    "You can scrape the latest data by yourself using ['scraper.py'](https://github.com/berodimas/one-month-one-dataset/blob/master/%5B2022-02%5D%20NFT%20Price%20Analysis%20and%20Regression/data/scraper.py) python script on 'data' folder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 1: Setup, Load, and Clean** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = ['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import neccessary libraries to load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the Dataset\n",
    "filepath = os.sep.join(\n",
    "    data_path + ['2022-03-01_niftyprice.csv'])\n",
    "df = pd.read_csv(filepath)\n",
    "df = df.sort_values(by=['Floor Cap'], ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the information from the data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If there's no volume change in past 7 days it might be newly listed\n",
    "## Therfore we gonna remove rows with 0 value on '7d%' column\n",
    "df = df[df['7d%'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill NaN value to 0\n",
    "df['24h Volume%'].fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We plan to choose top 200 NFT's collections\n",
    "df = df.drop(df.index[200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop all unnecessary columns\n",
    "df.drop(columns=['image_url', '24h supply%', '7d%',\n",
    "        'Floor Cap', 'Collection Name'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a quick look of the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-examine the information from the data\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create range section in describe table\n",
    "nft_df = df.copy()\n",
    "stat_df = nft_df.describe()\n",
    "stat_df.loc['range'] = stat_df.loc['max'] - stat_df.loc['min']\n",
    "stat_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import neccessary libraries to visualize data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_theme(style=\"dark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 10])\n",
    "sns.heatmap(data=nft_df.corr(), annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are 3 potential column to be our target variable\n",
    "## We gonna choose column with highest correlation with each others\n",
    "\n",
    "corr_column_target = ['Volume', 'Sales', '%Float']\n",
    "headers = ['Column Name', 'Sum of Correlations']\n",
    "df_corr_result = pd.DataFrame(columns=headers)\n",
    "i = 0\n",
    "\n",
    "for k in corr_column_target:\n",
    "    fields = list(nft_df.columns)\n",
    "    fields.remove(k)\n",
    "    y = (nft_df[k])\n",
    "    correlations = nft_df[fields].corrwith(y)\n",
    "    df_corr_result.loc[i] = [k, correlations.abs().sum()]\n",
    "    i += 1\n",
    "\n",
    "df_corr_result.sort_values(\n",
    "    by=['Sum of Correlations'], ascending=False, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Section Short Recap/Conclusion:\n",
    "* '%Float' have the highest correlation with each other.\n",
    "* We gonna compare each column as a target for Regression session. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 2: Simple Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check for unique variables on each features\n",
    "## Making sure that all of the columns were numerical\n",
    "\n",
    "nft_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data_columns = list(nft_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Numerical Data Columns EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize distribution on numerical features\n",
    "rows = len(numerical_data_columns)\n",
    "cols = 3\n",
    "\n",
    "fig = plt.figure(1, (18, rows*3))\n",
    "\n",
    "i = 0\n",
    "for feature in numerical_data_columns:\n",
    "\n",
    "    i += 1\n",
    "    ax1 = plt.subplot(rows, cols, i)\n",
    "    sns.kdeplot(data=nft_df, x=feature)\n",
    "    ax1.set_xlabel(None)\n",
    "    ax1.set_title(f'Distribution of {feature}')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    i += 1\n",
    "    ax2 = plt.subplot(rows, cols, i)\n",
    "    sns.violinplot(data=nft_df, x=feature)\n",
    "    ax2.set_xlabel(None)\n",
    "    ax2.set_title(f'{feature} - Swarm Plot')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    i += 1\n",
    "    ax3 = plt.subplot(rows, cols, i)\n",
    "    sns.boxplot(data=nft_df, x=feature, orient='h', linewidth=2.5)\n",
    "    ax3.set_xlabel(None)\n",
    "    ax3.set_title(f'{feature} - Box Plot')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find outliers using Tukey's method\n",
    "def tukey_outliers(x):\n",
    "    ## Tukey outliers are based on the boundaries defined by quantiles and IQR\n",
    "    q1 = np.percentile(x, 25)\n",
    "    q3 = np.percentile(x, 75)\n",
    "\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    lower_boundary = q1 - (iqr * 1.5)\n",
    "    upper_boundary = q3 + (iqr * 1.5)\n",
    "\n",
    "    outliers = x[(x < lower_boundary) | (x > upper_boundary)]\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the tukey outliers\n",
    "outlier_dict = {}\n",
    "for num_feature in numerical_data_columns:\n",
    "    outliers = tukey_outliers(nft_df[num_feature])\n",
    "    if len(outliers):\n",
    "        print(f\"-> {num_feature} has {len(outliers)} tukey outliers\")\n",
    "        outlier_dict[num_feature] = outliers\n",
    "    else:\n",
    "        print(f\"-> {num_feature} doesn't have any tukey outliers.\")\n",
    "        outlier_dict[num_feature] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Show the percentage of outliers\n",
    "\n",
    "for x in numerical_data_columns:\n",
    "    outliers = nft_df.loc[outlier_dict[x].index]\n",
    "    print(\"{} has {}% of outliers\".format(\n",
    "        x, round(len(outliers)/len(nft_df) * 100, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perform test whether a sample differs from a normal distribution\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "ALPHA = 0.05\n",
    "\n",
    "for col in nft_df:\n",
    "    stat, p = normaltest(nft_df[col].values)\n",
    "    print('{}: stat={}, p={}'.format(col, stat, p))\n",
    "    if p <= ALPHA:\n",
    "        print('Probably not Gaussian\\n')\n",
    "    else:\n",
    "        print('Probably Gaussian\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Data Columns EDA Short Recap/Conclusion:\n",
    "* '24h Owners% ' has the highest percentage of outliers with '17%'.\n",
    "* All of the columns were probably not normally distributed.\n",
    "* We gonna analyze skewness on next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 3: Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of scaler\n",
    "\n",
    "scalers = [\n",
    "    (MinMaxScaler(), \"MinMaxScaler\"),\n",
    "    (StandardScaler(), \"StandardScaler\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare result of skewness after scaled from each scaler\n",
    "for scaler, scaler_desc in scalers:\n",
    "    nft_df_fe = nft_df.copy()\n",
    "    skew_result = []\n",
    "    for column in numerical_data_columns:\n",
    "        nft_df_fe[[column]] = scaler.fit_transform(nft_df_fe[[column]])\n",
    "        skew_result.append({column: nft_df_fe[column].skew()})\n",
    "    print(\"Skew Result After \" + scaler_desc)\n",
    "    print(skew_result)\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Both were pretty same, so use any of those wouldn't be much problem\n",
    "nft_df_fe = nft_df.copy()\n",
    "for column in [numerical_data_columns]:\n",
    "    nft_df_fe[column] = StandardScaler().fit_transform(nft_df_fe[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display statistical value after scaling\n",
    "nft_df_fe.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take a quick look of the dataframe\n",
    "nft_df_fe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_limit = 0.75\n",
    "df_skew = nft_df_fe.copy()\n",
    "skew_vals = df_skew.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display skewness value for each columns\n",
    "skew_cols = (skew_vals\n",
    "             .sort_values(ascending=False)\n",
    "             .to_frame()\n",
    "             .rename(columns={0: 'Skew'})\n",
    "             .query('abs(Skew) > {}'.format(skew_limit)))\n",
    "\n",
    "skew_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create before-after tansformation graph\n",
    "skew_features = skew_cols.index.tolist()\n",
    "for field in skew_features:\n",
    "    # Create two \"subplots\" and a \"figure\" using matplotlib\n",
    "    fig, (ax_before, ax_after) = plt.subplots(1, 2, figsize=(25, 10))\n",
    "\n",
    "    # Create a histogram on the \"ax_before\" subplot\n",
    "    df_skew[field].hist(ax=ax_before)\n",
    "\n",
    "    # after_skew = np.sqrt(df_skew[field] + 0 - min(df_skew[field]))\n",
    "    after_skew = np.cbrt(df_skew[field])\n",
    "\n",
    "    # Apply a log transformation (numpy syntax) to this column\n",
    "    after_skew.hist(ax=ax_after)\n",
    "\n",
    "    # Formatting of titles etc. for each subplot\n",
    "    ax_before.set(title='before np.sqrt', ylabel='frequency', xlabel='value')\n",
    "    ax_after.set(title='after np.sqrt', ylabel='frequency', xlabel='value')\n",
    "    fig.suptitle('Field \"{}\"\\nBefore: {} | After: {}\\n'.format(\n",
    "        field, df_skew[field].skew(), after_skew.skew()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Apply transformation to the feature\n",
    "for column in numerical_data_columns:\n",
    "    df_skew[column] = np.cbrt(df_skew[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Transformation Short Recap/Conclusion:\n",
    "* Because there's negative value on skewed features ('Oldpeak'), we gonna use Cube Root as Feature Transformation approach. \n",
    "* After Feature Transformation with Cube Root method, all of the skewness seems getting close to 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 4: Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skew.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the Training and Test set with KFold\n",
    "## We gonna make 3 type of Training and Test set: %Float, Sales, and Volume\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_float = df_skew.drop(columns=[\"%Float\"])\n",
    "y_float = df_skew[\"%Float\"]\n",
    "\n",
    "X_sales = df_skew.drop(columns=[\"Sales\"])\n",
    "y_sales = df_skew[\"Sales\"]\n",
    "\n",
    "X_volume = df_skew.drop(columns=[\"Volume\"])\n",
    "y_volume = df_skew[\"Volume\"]\n",
    "\n",
    "kf = KFold(shuffle=True, random_state=72018, n_splits=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import neccessary libraries for modelling\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of transformers\n",
    "\n",
    "transformers = [\n",
    "    (PolynomialFeatures(degree=1), \"PolynomialFeatures (Degree 1)\"),\n",
    "    (PolynomialFeatures(degree=2), \"PolynomialFeatures (Degree 2)\"),\n",
    "    (PolynomialFeatures(degree=3), \"PolynomialFeatures (Degree 3)\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create list of models\n",
    "\n",
    "models = [\n",
    "    Lasso(max_iter=1000),\n",
    "    Ridge(max_iter=1000)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space_dict = {}\n",
    "\n",
    "search_space_dict['Lasso'] = {\n",
    "    'lasso__alpha': np.geomspace(0.001, 0.1, 50)\n",
    "}\n",
    "\n",
    "search_space_dict['Ridge'] = {\n",
    "    'ridge__alpha': np.geomspace(0.001, 0.1, 50)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create pipeline matrix\n",
    "\n",
    "pipelines_matrix = {}\n",
    "\n",
    "for transformer, transformer_desc in transformers:\n",
    "    pipelines_matrix[transformer_desc] = {}\n",
    "    print(transformer_desc)\n",
    "    for model in models:\n",
    "        print(\"            \", model.__class__.__name__)\n",
    "        pipelines_matrix[transformer_desc][model.__class__.__name__] = make_pipeline(\n",
    "            transformer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a function for performing cross validation of all algorithms\n",
    "## Fuction will return a dataframe with the result from each pipeline\n",
    "\n",
    "def cross_validator(X_train, y_train, pipelines_matrix):\n",
    "    i = 0\n",
    "    for transformer in pipelines_matrix:\n",
    "        print(\"----------------------\", transformer)\n",
    "        for model in pipelines_matrix[transformer]:\n",
    "            i += 1\n",
    "            print(\"     +++++++\", model)\n",
    "            startT = datetime.datetime.now()\n",
    "\n",
    "            pipeline = pipelines_matrix[transformer][model]\n",
    "\n",
    "            search_space = search_space_dict[model]\n",
    "            regressor = GridSearchCV(pipeline,\n",
    "                                search_space,\n",
    "                                scoring='neg_root_mean_squared_error',\n",
    "                                cv=kf)\n",
    "            regressor.fit(X_train, y_train)\n",
    "\n",
    "            print(\"          rmse: \", regressor.best_score_)\n",
    "\n",
    "            headers = ['transformer', 'model',\n",
    "                       'rmse', 'best_params']\n",
    "            dfResultsTemp = pd.DataFrame(columns=headers)\n",
    "            dfResultsTemp.loc[0] = [\n",
    "                transformer, model, regressor.best_score_, regressor.best_params_]\n",
    "\n",
    "            print(\"             exec time:\", datetime.datetime.now() -\n",
    "                  startT, datetime.datetime.now())\n",
    "\n",
    "            if i == 1:\n",
    "                data_concat = dfResultsTemp.copy()\n",
    "            else:\n",
    "                data_concat = pd.concat([data_concat, dfResultsTemp])\n",
    "\n",
    "    return data_concat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GridSearch with '%Float' as Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df = cross_validator(X_float, y_float, pipelines_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df.sort_values(by=['rmse'], ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('transformer', PolynomialFeatures(degree=3)),\n",
    "    ('model', Lasso(alpha=0.01151395399326447, max_iter=1000))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_float, y_float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = pipeline.predict(X_float)\n",
    "print(\n",
    "    f\"RMSE Score for Lasso Regression: {mean_squared_error(y_float, y_predict, squared=False)}\")\n",
    "print(f\"R2 Score for Lasso Regression: {r2_score(y_float, y_predict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(y_float, pipeline.predict(X_float),\n",
    "        marker='o', ls='', ms=3.0)\n",
    "\n",
    "lim = (0, y_float.max())\n",
    "\n",
    "ax.set(xlabel='Actual Confidence',\n",
    "       ylabel='Predicted Confidence',\n",
    "       xlim=lim,\n",
    "       ylim=lim,\n",
    "       title='Lasso Regression Results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GridSearch with 'Sales' as Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df = cross_validator(X_sales, y_sales, pipelines_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df.sort_values(by=['rmse'], ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('transformer', PolynomialFeatures(degree=3)),\n",
    "    ('model', Lasso(alpha=0.016768329368110076, max_iter=1000))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_sales, y_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = pipeline.predict(X_sales)\n",
    "print(\n",
    "    f\"RMSE Score for Lasso Regression: {mean_squared_error(y_sales, y_predict, squared=False)}\")\n",
    "print(f\"R2 Score for Lasso Regression: {r2_score(y_sales, y_predict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(y_sales, pipeline.predict(X_sales),\n",
    "        marker='o', ls='', ms=3.0)\n",
    "\n",
    "lim = (0, y_sales.max())\n",
    "\n",
    "ax.set(xlabel='Actual Confidence',\n",
    "       ylabel='Predicted Confidence',\n",
    "       xlim=lim,\n",
    "       ylim=lim,\n",
    "       title='Lasso Regression Results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **GridSearch with 'Volume' as Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df = cross_validator(X_volume, y_volume, pipelines_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df.sort_values(by=['rmse'], ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    ('transformer', PolynomialFeatures(degree=3)),\n",
    "    ('model', Lasso(alpha=0.0071968567300115215, max_iter=1000))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_volume, y_volume)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = pipeline.predict(X_volume)\n",
    "print(\n",
    "    f\"RMSE Score for Lasso Regression: {mean_squared_error(y_volume, y_predict, squared=False)}\")\n",
    "print(f\"R2 Score for Lasso Regression: {r2_score(y_volume, y_predict)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(10, 10))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(y_volume, pipeline.predict(X_volume),\n",
    "        marker='o', ls='', ms=3.0)\n",
    "\n",
    "lim = (0, y_volume.max())\n",
    "\n",
    "ax.set(xlabel='Actual Confidence',\n",
    "       ylabel='Predicted Confidence',\n",
    "       xlim=lim,\n",
    "       ylim=lim,\n",
    "       title='Lasso Regression Results')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Model Evaluation Short Recap/Conclusion:\n",
    "* After building model for 3 different target, 'Volume' got the best score with the highest R2 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**2022 | Dimas Adrian Mukti / [@berodimas](https://berodimas.netlify.app/)**"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "258094a0b5481bda05ae98aeeab472958eb7c2136bc8ae26c528a2a432605c0a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit (system)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
